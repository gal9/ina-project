{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'umap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gla\\Documents\\FAKS\\magisterij\\1_letnik\\2_semester\\introduction_to_network_analysis\\project\\ina-project\\experiment.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gla/Documents/FAKS/magisterij/1_letnik/2_semester/introduction_to_network_analysis/project/ina-project/experiment.ipynb#W1sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcluster\u001b[39;00m \u001b[39mimport\u001b[39;00m DBSCAN\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gla/Documents/FAKS/magisterij/1_letnik/2_semester/introduction_to_network_analysis/project/ina-project/experiment.ipynb#W1sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecomposition\u001b[39;00m \u001b[39mimport\u001b[39;00m PCA\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Gla/Documents/FAKS/magisterij/1_letnik/2_semester/introduction_to_network_analysis/project/ina-project/experiment.ipynb#W1sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mumap\u001b[39;00m \u001b[39mimport\u001b[39;00m UMAP\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gla/Documents/FAKS/magisterij/1_letnik/2_semester/introduction_to_network_analysis/project/ina-project/experiment.ipynb#W1sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gla/Documents/FAKS/magisterij/1_letnik/2_semester/introduction_to_network_analysis/project/ina-project/experiment.ipynb#W1sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m cross_val_score, cross_val_predict\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'umap'"
     ]
    }
   ],
   "source": [
    "# Data wrangling\n",
    "import numpy as np\n",
    "import pandas as pd  # Not a requirement of giotto-tda, but is compatible with the gtda.mapper module\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import open3d as o3d\n",
    "\n",
    "# Data viz\n",
    "from gtda.plotting import plot_point_cloud\n",
    "from gtda.plotting import plot_diagram\n",
    "\n",
    "# TDA magic\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.mapper import (\n",
    "    CubicalCover,\n",
    "    OneDimensionalCover,\n",
    "    make_mapper_pipeline,\n",
    "    Projection,\n",
    "    plot_static_mapper_graph,\n",
    "    plot_interactive_mapper_graph,\n",
    "    MapperInteractivePlotter\n",
    ")\n",
    "\n",
    "# ML tools\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import svm\n",
    "from src.feature_vectors import create_feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare cloud points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ply_files(folder):\n",
    "    files = list(filter(lambda file: file.split('.')[-1]=='ply', os.listdir(folder)))\n",
    "    files = list(map(lambda file: os.path.join(folder, file),files))\n",
    "    return files\n",
    "\n",
    "ply_files  = get_ply_files('data/tablesPly')\n",
    "ply_files += get_ply_files('data/chairsPly')\n",
    "ply_files += get_ply_files('data/octopusPly')\n",
    "ply_files += get_ply_files('data/spidersPly')\n",
    "\n",
    "# label files in group k with 1 and others with 0\n",
    "def label_groups(files, k):\n",
    "    group_sizes = [len([f for f in os.listdir('data/'+file) if f[-3:]=='ply']) for file in files]\n",
    "    group_sizes = [sum(group_sizes[:k]), group_sizes[k], sum(group_sizes[k+1:])]\n",
    "    labels = np.zeros(sum(group_sizes))\n",
    "    labels[group_sizes[0]:group_sizes[0]+group_sizes[1]] = 1        \n",
    "    return labels\n",
    "    \n",
    "files = ['tablesPly','chairsPly', 'octopusPly', 'spidersPly']\n",
    "labels = label_groups(files, 0)\n",
    "majority_classifier = (1 - sum(labels)/len(labels))\n",
    "print(\"Majority class classifier accuracy: %.3f\" % majority_classifier)\n",
    "\n",
    "pcd = [o3d.io.read_point_cloud(file) for file in ply_files]\n",
    "pcd = [np.asarray(pc.points) for pc in pcd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data for more accurate results\n",
    "shuffle_index = np.random.permutation(np.arange(0, len(labels)))\n",
    "labels = np.array(labels)\n",
    "pcd = np.array(pcd, dtype=object)\n",
    "labels = labels[shuffle_index]\n",
    "pcd = pcd[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistance and pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track connected components, loops, and voids\n",
    "homology_dimensions = [0, 1, 2]\n",
    "\n",
    "# Collapse edges to speed up H2 persistence calculation!\n",
    "persistence = VietorisRipsPersistence(\n",
    "    metric=\"euclidean\",\n",
    "    homology_dimensions=homology_dimensions,\n",
    "    n_jobs=6,\n",
    "    collapse_edges=True,\n",
    ")\n",
    "\n",
    "filter_func = Projection(columns=[0,1,2])\n",
    "# filter_func = PCA(n_components=2)\n",
    "# filter_func = UMAP(n_neighbors=5)\n",
    "\n",
    "cover = CubicalCover(n_intervals=4, overlap_frac=0.08)\n",
    "# cover = OneDimensionalCover(kind='uniform', n_intervals=10, overlap_frac=0.1)\n",
    "\n",
    "clusterer = DBSCAN(eps=10, metric=\"chebyshev\")\n",
    "\n",
    "n_jobs = 1\n",
    "\n",
    "pipe = make_mapper_pipeline(\n",
    "    filter_func=filter_func,\n",
    "    cover=cover,\n",
    "    clusterer=clusterer,\n",
    "    verbose=False,\n",
    "    n_jobs=n_jobs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature vector creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_feature_vectors = []\n",
    "feature_vectors = []\n",
    "start = time.time()\n",
    "for i, pc in enumerate(pcd):\n",
    "    print('\\r', f\"{int((i/len(pcd))*100)}%\", end=\"\")\n",
    "    e_fv, fv = create_feature_vector(pc, pipe, persistence)\n",
    "\n",
    "    entropy_feature_vectors.append(e_fv)\n",
    "    feature_vectors.append(fv)\n",
    "end = time.time()\n",
    "print(\"Time to compute create feature vectors:\", end - start, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(pcd), feature_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With homologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "num_features = len(feature_vectors[0])\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "best_scores = []\n",
    "# We take one homology and up to three other features\n",
    "\n",
    "for homology_idx in range(3):\n",
    "    final_fvs = []\n",
    "    \n",
    "    # First add homology and nothing else\n",
    "    for entropy_fv in entropy_feature_vectors:\n",
    "        final_fvs.append(entropy_fv[homology_idx])\n",
    "\n",
    "    # TODO add train and test\n",
    "    scores = cross_val_score(clf, final_fvs, labels, cv=10, scoring=\"roc_auc\")\n",
    "    y_pred = cross_val_predict(clf, final_fvs, labels, cv=10)\n",
    "    conf_mat = confusion_matrix(labels, y_pred)\n",
    "    best_scores.append((scores.mean(), \"h\"+str(homology_idx+1), conf_mat))\n",
    "    print(\"%0.2f accuracy with a standard deviation of %0.2f  %s\" % (scores.mean(), scores.std(), \"Homology\"+str(homology_idx+1)))\n",
    "\n",
    "    for number_of_additional_features in range(1,4):\n",
    "        combinations = list(itertools.combinations(range(num_features), number_of_additional_features))\n",
    "\n",
    "        # Loop through all posible feature subsets of size\n",
    "        for combination in combinations:\n",
    "            # print(combination)\n",
    "            final_fvs = []\n",
    "            # First add homology and a certain number of features\n",
    "            for fv_idx, entropy_fv in enumerate(entropy_feature_vectors):\n",
    "                extracted_fv = [x for x in entropy_fv[homology_idx]]\n",
    "\n",
    "                extracted_fv += [feature_vectors[fv_idx][i] for i in combination]\n",
    "\n",
    "                final_fvs.append(extracted_fv)\n",
    "\n",
    "            # TODO add train and test\n",
    "            scores = cross_val_score(clf, final_fvs, labels, cv=10, scoring=\"roc_auc\")\n",
    "            y_pred = cross_val_predict(clf, final_fvs, labels, cv=10)\n",
    "            conf_mat = confusion_matrix(labels, y_pred)\n",
    "            best_scores.append((scores.mean(), ''.join([str(x)+\" \" for x in combination]) + \"h\" + str(homology_idx+1), conf_mat))\n",
    "            # print(\"%0.2f accuracy with a standard deviation of %0.2f  %s\" % (scores.mean(), scores.std(), \"Combination of features \"+str(combination)))\n",
    "\n",
    "            # print(final_fvs)\n",
    "  \n",
    "best_scores.sort(reverse=True)\n",
    "\n",
    "with open('results.csv','ab') as out:\n",
    "    csv_out=csv.writer(out)\n",
    "    csv_out.writerow(['score','attributes', \"conf matrix\"])\n",
    "    for row in data:\n",
    "        csv_out.writerow(row)\n",
    "\n",
    "print(\"\\nAverage score: %0.4f\" % (sum([x for x, s, cm in best_scores]) / len(best_scores)))\n",
    "print(\"Best scores:\", [x[:2] for x in best_scores[:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = [(score, comb) for score, comb, cm in best_scores[:30] if score >= majority_classifier]\n",
    "fig = plt.figure(figsize = (20, 5))\n",
    "plt.bar([x[1] for x in data], [x[0] for x in data], color ='blue', width = 0.4)\n",
    "plt.bar(\"baseline\", majority_classifier, color ='red', width = 0.4)\n",
    "plt.ylim(0.5, best_scores[0][0] + 0.05)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.xlabel(\"Feature vectors\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Method accuracy using different combinations of parameters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "df_cm = pd.DataFrame(best_scores[0][2], index = ['True','False'], columns = ['True','False'])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(final_fvs, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "SVM = svm.LinearSVC()\n",
    "SVM.fit(X_train, y_train)\n",
    "SVM.predict(X_test)\n",
    "round(SVM.score(X_test,y_test), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without homologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_scores_2 = []\n",
    "for number_of_additional_features in range(1,6):\n",
    "    combinations = list(itertools.combinations(range(num_features), number_of_additional_features))\n",
    "\n",
    "    # Loop through all posible feature subsets of size\n",
    "    for combination in combinations:\n",
    "        # print(combination)\n",
    "        final_fvs = []\n",
    "        # First add homology and a certain number of features\n",
    "        for fv_idx, fv in enumerate(feature_vectors):\n",
    "            extracted_fv = [fv[i] for i in combination]\n",
    "\n",
    "            final_fvs.append(extracted_fv)\n",
    "\n",
    "        # TODO add train and test\n",
    "        scores = cross_val_score(clf, final_fvs, labels, cv=10, scoring=\"roc_auc\")\n",
    "        best_scores_2.append((scores.mean(), ''.join([str(x)+\" \" for x in combination])))\n",
    "        print(\"%0.2f accuracy with a standard deviation of %0.2f  %s\" % (scores.mean(), scores.std(), \"Combination of features \"+str(combination)))\n",
    "\n",
    "        # print(final_fvs)\n",
    "\n",
    "with open('results.csv','ab') as out:\n",
    "    csv_out=csv.writer(out)\n",
    "    for row in data:\n",
    "        csv_out.writerow(row)\n",
    "            \n",
    "best_scores_2.sort(reverse=True)\n",
    "print(\"\\nAverage score: %0.4f\" % (sum([x for x, s in best_scores_2]) / len(best_scores_2)))\n",
    "print(\"Best scores:\", best_scores_2[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(final_fvs, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "SVM = svm.LinearSVC()\n",
    "SVM.fit(X_train, y_train)\n",
    "SVM.predict(X_test)\n",
    "round(SVM.score(X_test,y_test), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(oob_score=True)\n",
    "rf.fit(X_test,y_test)\n",
    "\n",
    "print(f\"OOB score: {rf.oob_score_:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a1faa2b89841f2d81216e2b69e9280f066c9505dc0dde4939c230224824eed47"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
